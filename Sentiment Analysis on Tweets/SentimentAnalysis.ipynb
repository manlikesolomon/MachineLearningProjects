{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTzO-pUcMTjE"
      },
      "source": [
        "##Introduction\n",
        "In this notebook, I will walk you through the steps I took to analyze the sentiment of tweet sample data using natural language processing techniques. I started by importing the tweet sample data from the NLTK library, and then performed preprocessing tasks such as removing stop words and punctuations, tokenization, and stemming. Next, I built and trained a logistic regressor from scratch and utilized gradient descent to optimize the model. Finally, I tested the model with test data to evaluate its performance. Join me as we delve into the fascinating world of sentiment analysis and uncover insights from textual data using Python and the NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qr9Wd4Zgw6dz"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import re                        \n",
        "import string                    \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxXmgrKKxN0j",
        "outputId": "0e48e2f7-f3da-436a-fbcc-97d93afcfacc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import our dataset\n",
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPbm4VJKxbPo",
        "outputId": "161ddae2-e362-4b5c-93d6-6ca0b2027825"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# store positive and negative tweets in separate variables\n",
        "positive = twitter_samples.strings('positive_tweets.json')\n",
        "negative = twitter_samples.strings('negative_tweets.json')\n",
        "type(positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya-6cELFxsxl",
        "outputId": "e3aa47d5-ae50-4017-decf-099b219df221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#HappyBirthdayEmilyBett @emilybett :) Wishing you all the best you beautiful,sweet,talented,amazing… https://t.co/humtC1tr3I\n",
            "Ahh Fam @MeekMill :( #RespectLost http://t.co/NT25MYnGYd\n"
          ]
        }
      ],
      "source": [
        "# view samples of postive and negative tweets\n",
        "print(positive[70])\n",
        "print(negative[70])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxU72VMTyFkV"
      },
      "source": [
        "## Preprocessing Our Tweets for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6EudUZ0x7nQ",
        "outputId": "577913ea-f129-4ed0-96ad-cce800288ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
          ]
        }
      ],
      "source": [
        "# store a sample to test preprocessing techniques\n",
        "sample = positive[2277]\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjPS6gUWyeM6",
        "outputId": "ac53ad9f-2ebd-4f1e-9d60-a011dfdbc6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
          ]
        }
      ],
      "source": [
        "## preprocessing the sample tweet\n",
        "# remove hashtag(#) from tweet\n",
        "sample2 = re.sub('#','',sample)\n",
        "# remove oldstyle 'RT' from our tweets\n",
        "sample2 = re.sub(r'^RT[\\s]+','',sample2)\n",
        "# remove hyperlink\n",
        "sample2 = re.sub(r'https?://[^s\\n\\r]+','',sample2) \n",
        "print(sample2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DasiN_W8zMky",
        "outputId": "e9045723-9ad9-43ea-fd82-25ab559c6e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n"
          ]
        }
      ],
      "source": [
        "# tokenize and change to lowercase\n",
        "tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n",
        "tokenized_sample = tokenizer.tokenize(sample2)\n",
        "print(tokenized_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q7CIewA2oxA",
        "outputId": "975c81d1-1569-4464-acce-0f4bdb92215f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords') # download stopwords from nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ERyE8D0_Jq",
        "outputId": "396a77ca-f2cd-4f94-8745-8f2afda61a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
          ]
        }
      ],
      "source": [
        "stopwords_english = stopwords.words('english') #store english stopwords\n",
        "\n",
        "# remove stopwords and punctuations from tokenized tweets\n",
        "processed_tweets = []\n",
        "for word in tokenized_sample:\n",
        "  if word not in stopwords_english and word not in string.punctuation:\n",
        "    processed_tweets.append(word)\n",
        "\n",
        "print(processed_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm2NdIHQ1YEL",
        "outputId": "99ab204b-8916-458f-c9d8-8e9b43aff2d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
          ]
        }
      ],
      "source": [
        "# use stemming to reduce our vocabulary\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tweets = []\n",
        "for word in processed_tweets:\n",
        "  stem = stemmer.stem(word)\n",
        "  stemmed_tweets.append(stem)\n",
        "\n",
        "print(stemmed_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FXrjwwwD3OCv"
      },
      "outputs": [],
      "source": [
        "# define a function to preprocess our tweet dataset\n",
        "def process_tweet(tweet):\n",
        "  '''\n",
        "  input: \n",
        "    tweet - text to process\n",
        "  output:\n",
        "    tweet_clean - processed tweet\n",
        "  '''\n",
        "  stemmer = PorterStemmer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  # remove hashtag from our tweets\n",
        "  tweet = re.sub(r'#','',tweet)\n",
        "  # remove oldstyle 'RT' from our tweets\n",
        "  tweet = re.sub(r'^RT[\\s]+','',tweet)\n",
        "  # remove hyperlink from tweets\n",
        "  tweet = re.sub(r'https?://[^s\\n\\r]+','',tweet)\n",
        "  # remove stock market tickers like $GE\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  # tokenize tweets\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, \n",
        "                             strip_handles=True, reduce_len=True)\n",
        "  tweet = tokenizer.tokenize(tweet)\n",
        "  pro_tweet = []\n",
        "  # remove stopwords and punctions\n",
        "  for word in tweet:\n",
        "    if word not in stopwords_english and word not in string.punctuation:\n",
        "      pro_tweet.append(word)\n",
        "  # stemming\n",
        "  tweet_clean = []\n",
        "  for word in pro_tweet:\n",
        "    stem = stemmer.stem(word)\n",
        "    tweet_clean.append(stem)\n",
        "  \n",
        "  return tweet_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGdtsPYy7TUB",
        "outputId": "8731816b-ae9d-4f51-f1ec-5f94246a0989"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bad', 'man', 'badman', 'test']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test our function\n",
        "text = 'I am a bad man #badman testing https://mail.google.com/mail/u/0/#inbox'\n",
        "process_tweet(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWGNrGOJ8hw9"
      },
      "source": [
        "## Building a Frequency Dictionary.\n",
        "We will be building a word frequency dictionary that will be made of a key value pair of a word from our vocabulary and its negative and positive frequency. i.e {(word, sentiment): frequncy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l16mf7Xc9ekt",
        "outputId": "7b642083-03ef-4918-8191-87cff4323c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tweets: 10000\n",
            "All labels: 10000\n"
          ]
        }
      ],
      "source": [
        "# concate all our tweet samples\n",
        "tweets = positive + negative\n",
        "print(f'All tweets: {len(tweets)}')\n",
        "\n",
        "# create an array for sentiments; 1 for positive, 0 for negative\n",
        "labels = np.append(np.ones((len(positive),1)),np.zeros((len(negative),1)))\n",
        "print(f'All labels: {len(labels)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "DBySnykU-PJg"
      },
      "outputs": [],
      "source": [
        "# define a function to build frequency dictionaries\n",
        "def build_freq(tweets, labels):\n",
        "  '''\n",
        "  inputs:\n",
        "    tweets - vocabulary to of text\n",
        "    labels - corresponding sentiment\n",
        "  output:\n",
        "    freq_dict - frequency dictionary\n",
        "  '''\n",
        "  # create an empty dictionary \n",
        "  freq_dict = {}\n",
        "  yslist = np.squeeze(labels).tolist()\n",
        "  # loop thought tweets and labels to populate our dictiinary\n",
        "  for y, tweet in zip(yslist, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word,y)\n",
        "      if pair in freq_dict:\n",
        "        freq_dict[pair] += 1\n",
        "      else:\n",
        "        freq_dict[pair] = 1\n",
        "  return freq_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIpz8bUFBDpm",
        "outputId": "c3468c4b-c858-4350-8e11-cade754995eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length: 13391\n",
            "Datatype: <class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "# bulid our frequency dictionary\n",
        "freq = build_freq(tweets, labels)\n",
        "\n",
        "# print the length and datatype of freq\n",
        "print(f'Length: {len(freq)}')\n",
        "print(f'Datatype: {type(freq)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HXQEhyEBM48",
        "outputId": "a292ce57-738a-4897-9bbf-a088d08a9af7"
      },
      "outputs": [],
      "source": [
        "print(freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_izLhzYBq6w"
      },
      "source": [
        "##Building and Training A Logistic Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1QMkMasBvur",
        "outputId": "6804f76b-959b-42b4-c06b-b3410a15f19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets in training set: 8000\n",
            "Number of tweets in test set: 2000\n",
            "Number of labels in training set: 8000\n",
            "Number of labels in test set: 2000\n"
          ]
        }
      ],
      "source": [
        "# separating our train and test data\n",
        "train_pos = positive[:4000]\n",
        "train_neg = negative[:4000]\n",
        "test_pos = positive[4000:]\n",
        "test_neg = negative[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# store our labels in a variable\n",
        "train_y = np.append(np.ones((len(train_pos),1)),np.zeros((len(train_neg),1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos),1)),np.zeros((len(test_neg),1)), axis=0)\n",
        "\n",
        "print(f'Number of tweets in training set: {len(train_x)}')\n",
        "print(f'Number of tweets in test set: {len(test_x)}')\n",
        "print(f'Number of labels in training set: {len(train_y)}')\n",
        "print(f'Number of labels in test set: {len(test_y)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6Y4TcmzCY12",
        "outputId": "24e88e5f-df03-49e4-8964-3fd8f80315dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11617\n"
          ]
        }
      ],
      "source": [
        "# build a frequency dictionary for our train data\n",
        "freqs = build_freq(train_x,train_y)\n",
        "\n",
        "# view our dictionary\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mW4yA_efCyhi"
      },
      "outputs": [],
      "source": [
        "# define a sigmoid function for our logistic regressor\n",
        "def sigmoid(z):\n",
        "  '''\n",
        "  input:\n",
        "    z - a scalar or an array\n",
        "  output: \n",
        "    h - sigmoid of z\n",
        "  '''\n",
        "  h = 1/(1 + np.exp(-z))\n",
        "  return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Md23eebIEHlw"
      },
      "outputs": [],
      "source": [
        "# define a gradient descent function to train and optimize our module\n",
        "def gradientDescent(x, y, weight, alpha, epochs):\n",
        "  '''\n",
        "  input:\n",
        "    x - input vector\n",
        "    y - labels for input vector\n",
        "    weight - traing weight\n",
        "    alpha - learning rate\n",
        "    epochs - number of training iterations\n",
        "  output:\n",
        "    J - final cost of our training\n",
        "    weight - final weight vector\n",
        "  '''\n",
        "  m = len(x)\n",
        "  \n",
        "  for i in range(0, epochs):\n",
        "    # get the dot product of x and weight\n",
        "    z = np.dot(x, weight)\n",
        "    # get the sigmoid of z\n",
        "    h = sigmoid(z)\n",
        "    # calculate the cost function\n",
        "    J = (-1/m)*((np.dot(np.transpose(y),np.log(h))) + np.dot(np.transpose(1 - y),np.log(1-h)))\n",
        "    # update our model parameters\n",
        "    weight = weight - (alpha/m) * (np.dot(np.transpose(x),(h-y)))\n",
        "  J = float(J)\n",
        "  return J, weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eud6jgoQF3M1",
        "outputId": "63557ff7-0ec1-41ff-ede6-354b5b6f8173"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.6709497038162118,\n",
              " array([[4.10713435e-07],\n",
              "        [3.56584699e-04],\n",
              "        [7.30888526e-05]]))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing our function with random inputs and labels\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "temp_x = np.append(np.ones((10,1)), np.random.rand(10,2)*2000, axis=1)\n",
        "# label is a 10 x 1 array\n",
        "temp_y = (np.random.rand(10,1) > 0.35).astype('float')\n",
        "\n",
        "# test our gradent descent function\n",
        "gradientDescent(temp_x, temp_y, np.zeros((3,1)), 1e-8, 700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "cl7Z54TuGEZf"
      },
      "outputs": [],
      "source": [
        "# define a functions to extract features from our tweet\n",
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "  '''\n",
        "  input:\n",
        "    tweet - text to extract features\n",
        "    freqs - frequency dictionary\n",
        "  output:\n",
        "    x - feature_vector\n",
        "  '''\n",
        "  # process tweet\n",
        "  word_list = process_tweet(tweet)\n",
        "\n",
        "  # create vector x with zeros\n",
        "  x = np.zeros((1,3))\n",
        "\n",
        "  # add a bias term, 1\n",
        "  x[0,0] = 1\n",
        "\n",
        "  #loop through each word to populate our x vector\n",
        "  for word in word_list:\n",
        "    # positive frequency\n",
        "    x[0,1] += freqs.get((word,1),0)\n",
        "    # negative frequency\n",
        "    x[0,2] += freqs.get((word,0),0)\n",
        "  \n",
        "  return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifD1AEeRI97e",
        "outputId": "96ac5ab4-b890-48c5-ffd3-605750425b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1.000e+00, 3.051e+03, 6.100e+01]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test our function\n",
        "print(train_x[0])\n",
        "extract_features(train_x[0], freqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9MG5NMTJyZ1"
      },
      "source": [
        "##Train Our Logistic Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzrwB--qJCLR",
        "outputId": "063edf56-62c1-4780-8f07-54f56b73830f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cost after training is 0.23625149.\n",
            "The resulting vector of weights is [7e-08, 0.00052853, -0.00055649]\n"
          ]
        }
      ],
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, weight = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(weight)]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hjXWqHtkKLdb"
      },
      "outputs": [],
      "source": [
        "# define a function to make predictions using our weights\n",
        "def predict_tweet(tweet, freqs, weight):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - a string\n",
        "        freqs - a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta - (3,1) vector of weights\n",
        "    Output: \n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x, weight))\n",
        "    \n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WOXQPd2aKeAB"
      },
      "outputs": [],
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, weight, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"    \n",
        "    # create an empty list to store predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, weight)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    accuracy = np.mean(np.array(y_hat) == np.squeeze(test_y))\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRmp911SKjSa",
        "outputId": "7013817e-c30f-40d5-bb2c-733053e28b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ],
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, weight)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
